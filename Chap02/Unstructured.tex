%Some Unstructured Advice on Dissertation Writing
\chapter{Literature Survey}\label{Unstructured}
Outside the domain of physical formatting and layout, this document does not intend to foray into the contents of the dissertation to be written. That would influence contents and stubble creativity. The following unstructured guidelines, however, may sometime prove useful to the students and their guides, who are looking for a good model.

\par 
Following papers were studied

\section{Anomaly Detection: A Survey}

\subsection{Aspects of Anomaly Detection}

\begin{enumerate}
	\item \textit{Nature of Input Data} : Input
	is generally a collection of data instances. Each data instance contains some attribute values. Attributes can be of different type such as binary, categorical, or continuous.For nearest-neighbor-based techniques, the pairwise distance between instances might be
	provided in the form of a distance or similarity matrix. In such cases, techniques that
	require original data instances are not applicable.
	
	\item \textit{Type of Anomaly} :  
	Anomalies are of three types
	\begin{enumerate}
		\item \textit{Point Anomalies} : If a single data instance is considerd outlier to rest of the data points then it is called point anomaly. For examle in credit card  fraud detection the data instance the amount spent is very high compared to the normal range of expenditure
		for that person will be a point anomaly.
		
		\item \textit{Contextual Anomalies} : If a data point is considered anomaly in certain context but not otherwise then it  is called contextual or conditional anomaly. For example 1000 dollar expense per week may be considered anomaly in a normal week but not in Christmas week.
		
		\item \textit{Collective Anomalies} : If a collection of related data points are anomalous with respect to entire data set then they are termed as collective anomalies. Individually these data points may not be anomalous but taking together they are outliers. 
	\end{enumerate}



\item \textit{Data Labels}
Data labels for each instance shows whether it is a normal point or anomalous. Labeling is often done manually by a human expert which is clearly very expensive and time taking. Getting a training data set that contains the representative of all possible outliers is very difficult. Based on the extend of availability of labeled data, anomaly detection technique can operate in three modes

\begin{enumerate}
	
	\item \textit{Supervised Anomaly Detection} : Training data set that has labeled instances for both normal and anomalous class. Any new data instances is classified in one among these two classes. Major problem with this approach is to get labeled data and anomalous instances are very few as compared to normal instances. This imbalanced class distribution reduces the accuracy. 
	
	\item \textit{Semisupervised Anomaly Detection.} : Training data set has labeled instances for either normal or anomalous class only. It is very difficult to put representative instances for all possible outliers that can occur in the data.
	
	\item \textit{Unsupervised Anomaly Detection} : In unsupervised anomaly detection there is no training data. This works with the assumption that normal instances are far more than anomalies in test data.   
\end{enumerate}


\item \textit{Output of Anomaly Detection}

\begin{enumerate}
	\item \textit{Scores} : Score is assigned to each instance depending on the degree to which that instance is considered anomaly. Selecting the cutoff for anomaly is decided based on the number of top anomalous points.
	
	\item \textit{Label} : Binary label is assigned to each instance whether it is normal or anomalous.
\end{enumerate} 
	
	
\end{enumerate}


		 

\subsection{Anomaly Detection Techniques} 

\begin{enumerate}
	\item \textit {Nearest Neighbor Based } : Nearest neighbor based anomaly detection techniques works on the assumption that normal data points occur in dense clusters and oultliers lie far from their neighbors. It requires a distance or similarity
	measure defined between two data instances. Nearest neighbor based technique is broadly classified in two types
	
	\begin{enumerate}
		\item 
		\textit{Using Distance to $K^{th}$  Nearest Neighbor}
		The anomaly score of an instance is the distance between the point and it's $K^{th}$ nearest neighbor. A different approach to compute anomaly score is to count no of instances in a radius of d. 
		
		\item 
		\textit{Using Relative Density} : An instance that lies in a neighborhood of less density is considered outlier while an instance that lies in a dense neighborhood is considered normal. But this technique fails for dataset with varying densities. To address this problem the density of instances relative to their neighborhood is computed.
		
		\par To assign a relative density score, Local Outlier Factor was proposed. For any given data instance, the LOF score is equal to
		ratio of average local density of the k nearest neighbors of the instance and the local
		density of the data instance itself[2].
		
	\end{enumerate}



\item \textit{Clustering Based Anomaly Detection Techniques} : 

Clustering is grouping similar data points into clusters. Clustering is unsupervised. These techniques work on the assumption that
\begin{itemize}
	\item Normal points lie inside clusters and anomalous points does not belong to any cluster.
	
	\item Normal points lie closer to the cluster center while outliers lie far away from the cluster centers.
	
	\item Normal data points belongs to large and dense clusters while outliers belong to small and parse clusters.
\end{itemize} 

	

\end{enumerate}




\section{LOF: Identifying Density-Based Local Outliers}

the degree depends on how isolated the object is with respect to the
surrounding neighborhood. The outlier factor is local
in the sense that only a restricted neighborhood of each object is
taken into account.We show that for most objects in a cluster
their LOF are approximately equal to 1.

\par 
Local density based outlier detection is preferred because global density based techniques performs poorly for datasets with varying densities.
Consider the 2-D data set shown in Figure 1. Both points p1 and p2 should be detected as outliers. But if we consider global density, p2 will not be detected as outlier because of low density of cluster C1. For a point q in cluster C1, the distance between q and its nearest neighbor will always be greater than distance between p2 and its nearest neighbor.

\begin{figure}[t]
	\centering
	\includegraphics{chap01/localvsglobal.png}
	\caption{Result}
\end{figure}

\begin{itemize}
	\item \textbf{K-distance} :The distance between a data point p
	and its $K^{th}$ nearest neighbor (K-NN).
	
	\item \textbf{Reachability distance (reach-dist)} of a data point p
	with respect to another data point o
	
	\[ reach-dist_K(p,o)=max\{k-distance(o),d(p,o)\}  \]
	
	where d(p,o) is the euclidean distance between p
	and o.
	
	
	\item \textbf{Local Reachability Density(lrd)}  of a data point p
	
	
	\[  lrd_k(p) =  \bigg( \frac{1}{K} \sum_{o \in N_{(p,k)}} reach-dist(p,o)   \bigg)^{-1}  \]
	
	
	where $N_{(p,k)}$ is the set of k nearest neighbors of p.
	
	\item
	\textbf{Local Outlier Factor(LOF)}  of a data point p
	
	\[  LOF_K(p) = \frac{1}{K} \sum_{o \in N_{(p,k)}} \frac{lrd_K(o)}{lrd_K(p)}  \]
	
\end{itemize}

\par LOF is a score assigned to each instance that shows the degree of anomalous behavior. Instances inside a cluster have LOF value nearly equal to 1.


\section{Incremental Local Outlier Detection for Data Streams}
Incremental LOF for data streams refer to computation LOF score of newly added data points and updating the same for older data points. Outlier detection techniques are categorized into 4 types

\begin{enumerate}
	\item \textit{Statistical Approach} : the data points are typically modeled using a stochastic distribution, and points are labeled as outliers depending on their relationship with this model.
	
	\item \textit{Distance Based Approach} : detect outliers by computing distances among points.
	
	\item \textit{Profiling Methods} : Profiles of normal behavior is built and deviation from that are considered outliers.
	
	\item \textit{Model-based Approach} : First characterization of normal points using predictive models like neural networks and SVM. Deviation from these models are considered outliers.
	
	
\end{enumerate}

Static LOF algorithms can be applied to data stream is three ways. However thwese are computationally inefficient.

\begin{enumerate}
	\item \textit{Periodic LOF} : Static LOF is applied periodically on entire data set every time new data blocks are entered. The problem with such approach is that a data point which may be anomalous while added may not be detected as outlier later as many other points are added. they may create a cluster of their own and outliers will be missed. If LOF is computed each time a data point is added, the change is behavior can be detected at that moment.
	
	\item \textit{Supervised LOF} : K-distance, lrd and LOF of the data points are precomputed. Any new data point that arrives all these parameters are computed without updating for previous points. As a result the LOF accuracy decreases. 
	
	\item \textit{Iterated LOF} : Aplpy static LOF every time a new data point enters. This gives high accuracy at a cost of very high computational time. 
	
	
\end{enumerate} 

In Incremental approach, LOF score is computed as soon as a data enters. It is determined whether it is outlier or not. LOF of other data points are also updated. 

\subsection{Insertion}

K-distance, lrd, LOF are computed for each inserted data point and updated for affected points. 

\begin{algorithm}[h!]
	\caption{iLOF Insertion}
	\begin{algorithmic}
		\STATE  
		\STATE INPUT:  A data point $p_t$ at time t
		\STATE OUTPUT: LOF value LOF($p_t$)
		\STATE
		
		\STATE Compute KNN and K-distance of $p_t$
		
		\FOR {all o \in KNN($p_t$)}	
		
		\STATE Compute \texttt{reach-dist($p_t$,o)}
		
		\ENDFOR
		
		\STATE  $S_{update}$ \leftarrow Reverse KNNs of $p_t$
		
		\FOR{ all o \in $S_{update}$ and q \in $N_{(o,k)}$}
		
		\STATE Update K-distance(o) and reach-dist(q,o)
		
		\IF {o \in  $N_{(q,k)}$}
		
		\STATE $S_{update}$ \leftarrow  $S_{update}$ \cup q
		\ENDIF
		
		\ENDFOR
		
		\FOR { all o \in $S_{update}$ }
		
		\STATE Update lrd(o) and LOF(R-KNN(o,k))
		
		\ENDFOR
		\STATE Compute lrd($p_t$) and LOF($p_t$)
		\RETURN LOF
		
		
	\end{algorithmic}
\end{algorithm}


\section{Memory Efficient Incremental Local Outlier Factor (MiLOF)}

Local Outliers detection in data streams when limited memory is available. It is impractical to store all the instances of data streams. Some of the points has to be removed and summarized. Memory available is just sufficient to store K-Distance, lrd and LOF of m data points only.

The available m memory is divided in two parts of size b and c to store original points and summarized points respectively. When memory limit is reached, first ${\frac{b}{2}}$  data points are summarized to c clusters and deleted to free memory. If there already exists summarized data, the old and new cluster centers are merged. Hence at any time maximum memory used is b=m+c.
 Three steps of MiLOF are 

\subsection{Summarization}
Whenever memory reaches limit b, summarization phase is invoked. This phase summarizes first ${\frac{b}{2}}$ points, their K-distance, lrd and LOF and deletes these points from memory. Recent points are retained because data points might have evolved with time and recent points are most important. As the width of the summarization window decreases, it resembles iLOF. So MiLOF is direct generalization of iLOF.

\par 
Summarization  is explained in algorithm 2.
In summarization, if memory is reached first $\frac{b}{2}$ points are summarized to c clusters. K-distance, lrd and LOF for these cluster centers are computed by the following formulas.  

\begin{itemize}
	\item K-Distance of cluster center $v^i_j$ \in $V^i$ 
	
	
		\[  K-Distance(v^i_j) = \frac{\sum_{p \in C^i_j } K-Distance(p)}{| C^i |} \]
		
		
	 \item lrd of cluster center $v^i_j$ \in $V^i$ 
		
		
		\[  lrd_k(v^i_j) = \frac{\sum_{p \in C^i_j } lrd_k(p)}{| C^i |} \]
		
		\item LOF of cluster center $v^i_j$ \in $V^i$ 
		
		
		\[  LOF_k(v^i_j) = \frac{\sum_{p \in C^i_j } LOF_k(p)}{| C^i |} \]
	
\end{itemize}


\begin{algorithm}[h!]
	\caption{MiLOF}
	\begin{algorithmic}
		\STATE  
		\STATE INPUT:  A data point p 
		\STATE OUTPUT: LOF value LOF(p)
		\STATE
		
		\STATE Compute LOF of p 
		\IF {No of points=b}
			\STATE $C^i$ $\leftarrow$ First $\frac{b}{2}$ points
			\STATE ($V^i$,$N^i$) $\leftarrow$ C-means($C^i$)
			
			\FOR {all $v^i$ \in $V^i$} 
			  \STATE Compute avereage K-distance, lrd and LOF
			\ENDFOR
			
			\STATE Delete $C^i$
			
			\IF{i>0}
				\STATE (Z;W) $\leftarrow$ Weighted c-means($V^i$ U $V^{i-1}$ ,$N^i$ U $N^{i-1}$)
				
				\FOR {all z \in Z} 
					\STATE Compute avereage K-distance, lrd and LOF
				\ENDFOR 
				\STATE $V^{i-1}$ \leftarrow Z
				\STATE Delete $V^{i-1}$,Z
				
			\ENDIF	
		\ENDIF
		
		\STATE i $\leftarrow$ i+1
		\RETURN LOF
	\end{algorithmic}
\end{algorithm}

\subsection{Merging}

Summarization is performed every time new $\frac{b}{2}$ new data points arrives. Clustering these points gives c cluster centers $V^i$. These clusters are to be merged with old c cluster centers $V{i-1}$ so that finally only c centers are available. Cluster centers X={$V^i$ U $V^{i-1}$} are merged using a weighted clustering algorithm where weight of each center is no of objects in that cluster. 


\subsection{Revised Insertion}
Whenever a new data points arrives, LOF value for it is computed similar to iLOF with only difference that iLOF uses only data points to compute LOF where as in revised insertion we use both the data points and the cluster centers. While calculating the KNN, if any of the point is cluster center then it is assumed that all other nearest neighbor belongs to the same cluster. Hence distance from the point and that cluster center is taken as the K-distance for that point.  





